# üëã Welcome to Your Personal Dashboard!

Hi! I'm Claude, and I'm here to help you build a customized dashboard that tracks everything you care about on the internet.

Think of this as your personal command center - a single page that shows you:
- Breaking tech news from Hacker News
- Trending GitHub repositories
- Latest research papers from arXiv
- Fashion drops from Hypebeast
- **...or whatever YOU want to track!**

---

## üöÄ Let's Build Your Dashboard!

I'll ask you **3 simple questions** to customize your dashboard, then I'll build it automatically (~10 minutes).

### Ready to start?

Just say:
```
"Let's get started" or "Help me set up my dashboard"
```

**Here's what I'll ask:**
1. **What do you want to track?** - I'll suggest sources based on your interests (tech, finance, fashion, etc.)
2. **What theme do you like?** - I'll show you 6 options (dark, light, colorful, minimal, etc.)
3. **How often should it update?** - I'll recommend a schedule based on your sources

Then I build everything while you watch!

---

### Not Ready Yet? Ask Me Anything!

Have questions before we start? Ask away:

- "Can you scrape [specific website]?"
- "What themes are available?"
- "Can I track Reddit?"
- "How does the auto-update work?"
- "What if a scraper breaks?"

When you're ready, just say **"Let's get started"** ‚ú®

---

## Step 1: What Do You Want to Track?

This is the fun part! Tell me what interests you, and I'll create custom scrapers to fetch that data.

**IMPORTANT:** I will ONLY build scrapers for sources you explicitly request. I will NOT assume you want any "built-in" sources unless you specifically ask for them.

### üí° Popular Ideas

Not sure what to track? Here are ideas organized by interest:

**üë®‚Äçüíª For Developers & Tech Enthusiasts**
- Product Hunt daily launches
- Stack Overflow trending questions
- Dev.to top posts
- Reddit r/programming frontpage
- npm trending packages

**üí∞ For Finance & Crypto**
- CoinMarketCap top movers
- Stock market indices (S&P 500, NASDAQ)
- Crypto fear & greed index
- Economic calendar events
- Reddit r/wallstreetbets hot posts

**üì∞ For News Junkies**
- Twitter/X trending topics
- Reddit r/worldnews or r/news
- TechCrunch latest articles
- Medium trending stories
- Wikipedia trending articles

**üõçÔ∏è For Shopping & Deals**
- Amazon Gold Box deals
- Slickdeals frontpage
- Etsy trending items
- Upcoming sneaker releases (StockX, GOAT)
- Woot daily deals

**üé¨ For Entertainment**
- IMDb upcoming releases
- Letterboxd popular reviews
- Songkick concert listings
- Twitch top streams
- Podcast charts (Apple, Spotify)

**üìö For Learning**
- Coursera new courses
- YouTube trending in your categories
- Khan Academy new content
- Udemy deals
- arXiv in specific fields (physics, math, biology)

### üé® Or Track Something Unique!

Tell me any website and I'll build a scraper for it. Examples:
- "Track my local restaurant's reservations on OpenTable"
- "Monitor when concert tickets go on sale"
- "Watch for new job postings on a specific site"
- "Track prices on a specific product"

**Just tell me:**
```
"I want to track [website/topic]"
```

---

## Step 2: Choose Your Theme

Your dashboard should look good! Pick a theme or describe what you want:

### üé® Pre-Made Themes

**1. Hacker News Orange** (Default)
```
Classic tech aesthetic
- Background: Off-white (#f6f6ef)
- Accent: Orange (#ff6600)
- Font: Monospace
- Vibe: Retro terminal, minimal
```
Perfect for: Developers, tech enthusiasts

**2. Minimal Dark**
```
Modern dark mode
- Background: Dark grey (#1a1a1a)
- Accent: Teal (#00d9ff)
- Font: Sans-serif
- Vibe: Clean, professional
```
Perfect for: Night owls, extended reading

**3. Light & Airy**
```
Clean and spacious
- Background: White (#ffffff)
- Accent: Blue (#0066cc)
- Font: System default
- Vibe: Professional, easy on eyes
```
Perfect for: Daytime use, sharing

**4. Synthwave**
```
80s retro vibes
- Background: Deep purple (#1e1e2e)
- Accent: Neon pink (#ff79c6)
- Font: Rounded sans-serif
- Vibe: Fun, eye-catching
```
Perfect for: Standing out, creative projects

**5. Nord Theme**
```
Calm and sophisticated
- Background: Blue-grey (#2e3440)
- Accent: Muted blue (#88c0d0)
- Font: Sans-serif
- Vibe: Calm, nordic minimalism
```
Perfect for: Focus, productivity

**6. Solarized Light**
```
Scientifically designed colors
- Background: Cream (#fdf6e3)
- Accent: Blue (#268bd2)
- Font: Monospace
- Vibe: Easy on eyes, balanced
```
Perfect for: Long reading sessions

### üåà Custom Theme

Want something different? Tell me:
- Your favorite colors
- A website you like the look of
- Your company/brand colors
- Or just describe the vibe: "I want it to look like..."

**Just tell me:**
```
"I want [theme name]" or "Make it look like [description]"
```

---

## Step 3: Set Update Frequency

How often should your dashboard fetch fresh data?

### ‚è±Ô∏è Frequency Options

**Very Frequent (Real-time feel)**
```
Every 15 minutes - Breaking news, stock prices, live events
Every 30 minutes - Social media, active communities
```
‚ö†Ô∏è Higher server load, respect rate limits

**Regular Updates (Recommended)**
```
Every 1 hour   - General news, tech sites (RECOMMENDED)
Every 3 hours  - Blog posts, articles
Every 6 hours  - Shopping deals, releases
```
‚úÖ Good balance of freshness and efficiency

**Scheduled Updates**
```
Twice daily    - Morning (8am) and Evening (6pm)
Daily at 7am   - Overnight updates, market opens
Weekly         - Fashion drops, curated content
```
üí° Best for content that changes on a schedule

### üìÖ Smart Scheduling Tips

**Match the source:**
- Hacker News / Reddit ‚Üí Every 1-3 hours
- Research papers ‚Üí Daily
- Fashion drops ‚Üí Weekly
- Stock prices ‚Üí Every 15-30 minutes
- Blog posts ‚Üí Every 6 hours

**Consider your timezone:**
Schedule updates before you typically check the dashboard

**Avoid over-scraping:**
Be respectful of websites - don't hammer them with requests

**Just tell me:**
```
"Update every [frequency]" or "Update daily at [time]"
```

---

## üéØ Quick Examples

Not sure how to phrase it? Here are complete examples:

**Example 1: Tech Enthusiast**
```
"Set up my dashboard with Hacker News, GitHub Trending, and Product Hunt.
Use the dark theme and update every hour."
```

**Example 2: Crypto Trader**
```
"I want to track Bitcoin price from CoinMarketCap, crypto news from Reddit
r/cryptocurrency, and fear & greed index. Synthwave theme, update every 15 minutes."
```

**Example 3: Researcher**
```
"Track arXiv papers in machine learning, Papers with Code trending, and
Google Scholar alerts. Solarized light theme, update daily at 7am."
```

**Example 4: Fashion Fan**
```
"Keep Hypebeast weekly drops, add StockX upcoming releases and Grailed trending.
Light & airy theme, update twice daily."
```

**Example 5: Custom**
```
"I want a dashboard like Hacker News but for design. Track Dribbble popular shots,
Behance trending, and Designer News. Orange theme, update every 3 hours."
```

---

## ü§ñ What Happens Next?

Once you tell me what you want, here's what I'll do:

### 0. Test Scrapeability FIRST (1-2 minutes) ‚ö†Ô∏è

**Before building anything, I'll validate that the website can actually be scraped:**

```
"Can you scrape Amazon?"
‚Üí I'll write a quick test script to see if data is accessible
‚Üí Show you a sample of the data I can extract
‚Üí Tell you the success rate and any challenges
‚Üí Then we decide together if it's worth building
```

**Why test first?**
- Saves time (no point building infrastructure for broken scrapers)
- Sets realistic expectations (some sites are impossible to scrape)
- Lets you pivot to alternatives if needed (RSS feeds, APIs, aggregators)

**What I test:**
1. Can I access the page? (200 status vs 403 blocked)
2. Does it require JavaScript rendering? (static HTML vs dynamic)
3. Is there bot detection? (Cloudflare, CAPTCHAs, rate limits)
4. Can I extract the data you want? (show you a sample)
5. How reliable is it? (test 2-3 times to check consistency)

**After testing, I'll tell you:**
- ‚úÖ **Green light** - "This will work great! Here's sample data..."
- ‚ö†Ô∏è **Yellow light** - "This will work but expect 70-80% success rate due to [challenge]"
- ‚ùå **Red light** - "This is blocked/impossible, here are alternatives..."

**Then you decide:**
- Proceed with building the scraper
- Use an alternative source I suggest
- Combine multiple approaches (primary + backup)

### 1. Build Your Scrapers (2-5 minutes)

**I'll choose the right scraping approach for each website:**

#### üéØ Decision Tree

**Simple API/Fetch (Fastest, Most Reliable)**
```
Use when: Website has a public API or returns clean JSON/HTML
Examples: Hacker News, GitHub API, arXiv RSS
Benefits: Fast, lightweight, no browser needed
Method: Direct HTTP requests with fetch/axios
```
‚úÖ **Preferred method when available** - no overkill, just clean data extraction

**Headless Browser - Stealth Mode (Advanced Anti-Detection)**
```
Use when: Website has moderate bot detection but loads content via JavaScript
Examples: Product Hunt, Twitter aggregators, Reddit
Benefits: Bypasses basic bot detection, handles dynamic content
Method: playwright-extra + stealth plugin + smart utilities
```
‚úÖ **Uses pre-installed web-automation skill** with anti-detection features:
- `playwright-extra` with stealth plugin (masks automation fingerprints)
- Human-like interactions (`humanType`, `humanClick`, `randomDelay`)
- Cookie persistence (`saveCookies`/`loadCookies`) for session reuse
- Cloudflare detection (`detectCloudflare`) with 90s auto-solve
- Retry logic with exponential backoff
- Smart timeouts based on traffic patterns

**Visible Browser - Brute Force (Nuclear Option)**
```
Use when: Website has aggressive bot detection (Amazon, Cloudflare, etc.)
Examples: Amazon, heavily protected e-commerce sites
Benefits: Harder to detect (real browser behavior), manual CAPTCHA solving
Method: headless: false + all stealth techniques + human intervention if needed
```
‚úÖ **Last resort** - visible browser window, you can manually solve CAPTCHAs if needed

**What gets built:**
- Smart scrapers that auto-detect Cloudflare/bot challenges
- Clean JSON data saved to `drops/[source].json`
- Production-ready error handling and logging
- Session persistence (faster subsequent runs)

**üñºÔ∏è IMPORTANT: Image Extraction**

When building scrapers, ALWAYS look for and extract image URLs when available:

**For RSS/Atom feeds:**
- Check `<content:encoded>` field for `<img>` tags
- Check `<media:content>` or `<enclosure>` tags
- Example: Slickdeals RSS has images in `content:encoded`

**For API responses:**
- Look for `image`, `thumbnail`, `picture`, `photo` fields
- Check nested objects (e.g., `product.image.url`)

**For HTML scraping:**
- Extract `og:image` meta tags
- Find product images in data attributes or JSON-LD
- Grab thumbnail URLs from CSS backgrounds

**Why images matter:**
- Users love visual previews (hover to see product images)
- Makes the dashboard more engaging and useful
- Helps users quickly identify items of interest

**How to add hover preview:**
1. Add `image` field to scraped data
2. In frontend render, add `data-image="${image}"` attribute to title links
3. Call `addImageHoverListeners()` after rendering
4. CSS and JS for tooltip are already built-in!

**Example scraper code:**
```javascript
// Extract image from content:encoded
let imageUrl = null;
if (contentMatch) {
  const content = contentMatch[1];
  const imgMatch = content.match(/<img[^>]+src="([^"]+)"/);
  if (imgMatch) {
    imageUrl = imgMatch[1].replace(/&amp;/g, '&');
  }
}

return {
  title,
  link,
  description,
  image: imageUrl,  // ‚Üê Always include this!
  // ... other fields
};
```

### 2. Update Your Backend (1 minute)
I'll modify the Flask API to:
- Read your new data sources
- Serve them via REST endpoints
- Handle caching properly

### 3. Update Your Frontend (2 minutes)
I'll update the UI to:
- Display your data beautifully
- Apply your chosen theme
- Format everything nicely (timestamps, numbers, links)

### 4. Set Up Automation (1 minute)
I'll create cron jobs to:
- Run your scrapers automatically
- Update data on your schedule
- Log results for monitoring

### 5. Test Everything (1 minute)
I'll run a test to make sure:
- Scrapers work correctly
- Data displays properly
- Theme looks good
- Refresh button works

**Total time: ~10 minutes**

### 6. Here Are Your Links! üéâ

When setup is complete, I'll give you:

**Local Access (same device):**
```
http://localhost:5000
```

**Local Network Access (other devices on your WiFi):**
```
http://192.168.x.x:5000
```
(I'll detect your IP automatically)

**Public HTTPS Access (anywhere on internet):**
```
https://{your-subdomain}.devices.pamir.ai/distiller/proxy/5000/
```
(Works through Distiller proxy - already configured!)

**üîç How to Find Your Subdomain:**

To find your public URL subdomain, I will check the frpc configuration:
```bash
cat ~/.frpc/frpc.toml | grep subdomain
```

Look for the line: `subdomain = "your-name"`

Your public URL will be: `https://your-name.devices.pamir.ai/distiller/proxy/5000/`

**Example:** If `subdomain = "naruto"`, your URL is:
```
https://naruto.devices.pamir.ai/distiller/proxy/5000/
```

You can bookmark any of these URLs and access your dashboard from anywhere!

---

## üÜò Need Help Deciding?

### Not Sure What to Track?

**Ask yourself:**
- What websites do I check every morning?
- What newsletters do I subscribe to?
- What do I Google regularly?
- What would save me time if it was in one place?

**Or ask me:**
```
"What should I track if I'm interested in [topic]?"
"Show me examples of dashboards for [profession/hobby]"
```

### Can't Pick a Theme?

**Ask yourself:**
- When will I use this most? (Day/night)
- What colors do I like?
- Do I want it professional or fun?

**Or just say:**
```
"Surprise me with a theme!" (I'll pick based on your content)
```

### Not Sure About Update Frequency?

**Start with:**
- **Every hour** for most sources (you can change it later)
- **Twice daily** if you only check morning/evening
- **As needed** and just use the manual refresh button

---

## üé¨ Ready to Start?

Just say one of these to begin:

```
"I'm ready to customize my dashboard!"
```
(I'll ask you the three questions)

```
"Set up my dashboard with [topics], [theme], and update [frequency]"
```
(I'll do it all at once)

```
"Help me decide what to track"
```
(I'll ask about your interests and suggest sources)

```
"Show me a demo setup"
```
(I'll set up a sample dashboard so you can see how it works)

---

## üìù Advanced Options

Once your dashboard is running, you can always ask me to:

### Add Features
- "Add search/filter functionality"
- "Add email notifications when certain items appear"
- "Add data export (CSV/JSON)"
- "Add dark/light mode toggle"
- "Track historical data (graphs/charts)"

### Customize Further
- "Change just the colors but keep the layout"
- "Add custom CSS animations"
- "Make the cards bigger/smaller"
- "Add images/thumbnails for items"

### Manage Data
- "Show me the scrapers' debug output"
- "Why isn't [source] updating?"
- "Delete old data older than 7 days"
- "Show me what websites are being slow"

### Scale Up
- "Add 10 more sources"
- "Set different update frequencies per source"
- "Add authentication/password protection"
- "Set up multiple dashboards for different topics"

---

## üí¨ I'm Here to Help!

Think of me as your pair programmer for this project. Ask me anything:

- ‚úÖ "Can you scrape [specific website]?"
- ‚úÖ "Why is [source] not working?"
- ‚úÖ "How do I change [feature]?"
- ‚úÖ "Make it look more like [website]"
- ‚úÖ "Add a section for [new content type]"

Let's build something awesome together! üöÄ

---

## üîß Technical Reference: Web Automation Skill

### Overview

This project includes a pre-installed **web-automation skill** at `.claude/skills/web-automation/` that provides production-ready browser automation utilities. When building scrapers, I'll automatically use these tools to ensure reliability and anti-detection.

### Why This Skill Exists

**Problem:** Basic Playwright scripts fail on modern websites due to:
- Bot detection (Amazon, Cloudflare, etc.)
- JavaScript-heavy pages that don't render immediately
- Rate limiting and IP blocking
- Cookie/session management complexity

**Solution:** The web-automation skill provides battle-tested utilities from production bots.

### When I Use Each Approach

#### ‚úÖ Simple Fetch/API (No Skill Needed)
```javascript
// Example: Hacker News API
const response = await fetch('https://hacker-news.firebaseio.com/v0/topstories.json');
const data = await response.json();
```
**When:** Site has public API or returns static HTML/JSON
**Speed:** ‚ö° Instant (no browser startup)
**Reliability:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (no bot detection)

#### ‚úÖ Headless Browser + Stealth (Uses Skill)
```javascript
const { chromium } = require('playwright-extra');
const stealth = require('puppeteer-extra-plugin-stealth')();
const { humanType, detectCloudflare, saveCookies } = require('./.claude/skills/web-automation/lib/utils.js');

chromium.use(stealth);
const browser = await chromium.launch({ headless: true });

// Smart Cloudflare detection
await detectCloudflare(page, context, 'page load', {
  maxWaitSeconds: 90,
  cookiePath: 'session.json'
});

// Human-like typing
await humanType(page, '#search', 'query', { delay: 50 });
```
**When:** Site has moderate bot detection (Product Hunt, Reddit aggregators)
**Speed:** üê¢ ~5-10 seconds (browser startup + page load)
**Reliability:** ‚≠ê‚≠ê‚≠ê‚≠ê (bypasses most detection)

#### ‚úÖ Visible Browser + Manual Intervention (Nuclear Option)
```javascript
const browser = await chromium.launch({
  headless: false,  // You can see the browser
  args: ['--disable-blink-features=AutomationControlled']
});

// If CAPTCHA appears, you can solve it manually
console.log('Waiting for manual CAPTCHA solve...');
await page.waitForTimeout(30000);
```
**When:** Site has aggressive detection (Amazon, Cloudflare protected sites)
**Speed:** üêå ~10-30 seconds (+ manual intervention time)
**Reliability:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (humans always win)

### Available Utilities (from web-automation skill)

```javascript
const utils = require('./.claude/skills/web-automation/lib/utils.js');

// Session persistence (avoid repeated logins)
await utils.saveCookies(context, 'session.json');
await utils.loadCookies(context, 'session.json');

// Human-like interactions (avoid bot detection)
await utils.humanType(page, '#input', 'text', { delay: 50 });
await utils.humanClick(page, 'button');
await utils.randomDelay(1000, 2500); // Random wait time

// Smart Cloudflare handling
await utils.detectCloudflare(page, context, 'location', {
  maxWaitSeconds: 90,
  pollIntervalSeconds: 10,
  cookiePath: 'session.json'
});

// Retry logic with exponential backoff
const result = await utils.retryWithBackoff(async () => {
  await page.waitForSelector('.data', { timeout: 10000 });
  return await page.textContent('.data');
}, { maxRetries: 3 });

// Safe interactions with auto-retry
await utils.safeClick(page, 'button', { retries: 3, timeout: 10000 });
await utils.captureScreenshot(page, 'debug.png');
```

### What Makes Scrapers "Production-Ready"

When I build scrapers using the web-automation skill, they include:

1. **Anti-Detection**
   - `playwright-extra` with stealth plugin (masks automation fingerprints)
   - Human-like typing/clicking with random delays
   - Realistic user agents and viewport sizes

2. **Error Handling**
   - Automatic retries with exponential backoff
   - Screenshots captured on failures
   - Graceful degradation (partial data better than no data)

3. **Performance**
   - Cookie persistence (skip login flows on subsequent runs)
   - Smart timeouts (longer during peak traffic hours)
   - Parallel scraping where possible

4. **Bot Protection**
   - Cloudflare auto-detection with 90s polling
   - Block tracking (logs consecutive/total blocks)
   - Automatic session persistence after challenge clears

5. **Observability**
   - Detailed console logging with timestamps
   - JSON output for easy debugging
   - Error screenshots saved automatically

### Example: How I Choose the Approach

**User Request:** "Track Product Hunt, Amazon deals, and GitHub trending"

**My Decision Process:**

1. **GitHub Trending** ‚Üí Simple Fetch/API
   - GitHub has a trending page with clean HTML
   - No bot detection
   - Fast and reliable
   - ‚úÖ Use basic fetch or axios

2. **Product Hunt** ‚Üí Headless + Stealth
   - JavaScript-heavy page
   - Moderate bot detection
   - Data loads dynamically
   - ‚úÖ Use playwright-extra + stealth + utilities

3. **Amazon** ‚Üí Visible Browser (if needed)
   - Aggressive bot detection
   - Cloudflare challenges common
   - May require CAPTCHA solving
   - ‚úÖ Start with headless + stealth, fallback to headless: false if blocked

### Files Structure

```
.claude/skills/web-automation/
‚îú‚îÄ‚îÄ SKILL.md                    # Full documentation
‚îú‚îÄ‚îÄ lib/
‚îÇ   ‚îî‚îÄ‚îÄ utils.js                # Production utilities I'll use
‚îú‚îÄ‚îÄ examples/
‚îÇ   ‚îú‚îÄ‚îÄ login-flow.js           # Session persistence example
‚îÇ   ‚îú‚îÄ‚îÄ cloudflare-handler.js   # Bot protection example
‚îÇ   ‚îî‚îÄ‚îÄ data-extraction.js      # Scraping with retry logic
‚îî‚îÄ‚îÄ node_modules/
    ‚îú‚îÄ‚îÄ playwright-extra/       # Enhanced Playwright
    ‚îî‚îÄ‚îÄ puppeteer-extra-plugin-stealth/  # Stealth plugin
```

### Why You Don't Need to Worry

When you ask me to scrape a website:
1. I automatically check if `.claude/skills/web-automation/` exists
2. I choose the right approach (fetch vs headless vs visible)
3. I use the utilities to handle edge cases
4. I test the scraper and show you the results

You just tell me **what** to scrape, and I handle **how** to scrape it reliably.

---

**So... what would you like to track?** üëÄ
