# ðŸ‘‹ Welcome to Your Personal Dashboard!

Hi! I'm Claude, and I'm here to help you build a customized dashboard that tracks everything you care about on the internet.

Think of this as your personal command center - a single page that shows you:
- Breaking tech news from Hacker News
- Trending GitHub repositories
- Latest research papers from arXiv
- Fashion drops from Hypebeast
- **...or whatever YOU want to track!**

---

## ðŸš€ Let's Build Your Dashboard!

I'll ask you **3 simple questions** to customize your dashboard, then I'll build it automatically (~10 minutes).

### Ready to start?

Just say:
```
"Let's get started" or "Help me set up my dashboard"
```

**Here's what I'll ask:**
1. **What do you want to track?** - I'll suggest sources based on your interests (tech, finance, fashion, etc.)
2. **What theme do you like?** - I'll show you 6 options (dark, light, colorful, minimal, etc.)
3. **How often should it update?** - I'll recommend a schedule based on your sources

Then I build everything while you watch!

---

### Not Ready Yet? Ask Me Anything!

Have questions before we start? Ask away:

- "Can you scrape [specific website]?"
- "What themes are available?"
- "Can I track Reddit?"
- "How does the auto-update work?"
- "What if a scraper breaks?"

When you're ready, just say **"Let's get started"** âœ¨

---

## Step 1: What Do You Want to Track?

This is the fun part! Tell me what interests you, and I'll create custom scrapers to fetch that data.

### ðŸŽ¯ Already Built-In (Ready to Use)

Your dashboard comes with these sources pre-installed:

- **Hacker News** - Top tech stories and discussions
- **GitHub Trending** - Hottest open source projects
- **arXiv** - Latest computer science research papers
- **Hypebeast** - Weekly fashion and streetwear drops

### ðŸ’¡ Popular Custom Ideas

Not sure what to track? Here are ideas organized by interest:

**ðŸ‘¨â€ðŸ’» For Developers & Tech Enthusiasts**
- Product Hunt daily launches
- Stack Overflow trending questions
- Dev.to top posts
- Reddit r/programming frontpage
- npm trending packages

**ðŸ’° For Finance & Crypto**
- CoinMarketCap top movers
- Stock market indices (S&P 500, NASDAQ)
- Crypto fear & greed index
- Economic calendar events
- Reddit r/wallstreetbets hot posts

**ðŸ“° For News Junkies**
- Twitter/X trending topics
- Reddit r/worldnews or r/news
- TechCrunch latest articles
- Medium trending stories
- Wikipedia trending articles

**ðŸ›ï¸ For Shopping & Deals**
- Amazon Gold Box deals
- Slickdeals frontpage
- Etsy trending items
- Upcoming sneaker releases (StockX, GOAT)
- Woot daily deals

**ðŸŽ¬ For Entertainment**
- IMDb upcoming releases
- Letterboxd popular reviews
- Songkick concert listings
- Twitch top streams
- Podcast charts (Apple, Spotify)

**ðŸ“š For Learning**
- Coursera new courses
- YouTube trending in your categories
- Khan Academy new content
- Udemy deals
- arXiv in specific fields (physics, math, biology)

### ðŸŽ¨ Or Track Something Unique!

Tell me any website and I'll build a scraper for it. Examples:
- "Track my local restaurant's reservations on OpenTable"
- "Monitor when concert tickets go on sale"
- "Watch for new job postings on a specific site"
- "Track prices on a specific product"

**Just tell me:**
```
"I want to track [website/topic]"
```

---

## Step 2: Choose Your Theme

Your dashboard should look good! Pick a theme or describe what you want:

### ðŸŽ¨ Pre-Made Themes

**1. Hacker News Orange** (Default)
```
Classic tech aesthetic
- Background: Off-white (#f6f6ef)
- Accent: Orange (#ff6600)
- Font: Monospace
- Vibe: Retro terminal, minimal
```
Perfect for: Developers, tech enthusiasts

**2. Minimal Dark**
```
Modern dark mode
- Background: Dark grey (#1a1a1a)
- Accent: Teal (#00d9ff)
- Font: Sans-serif
- Vibe: Clean, professional
```
Perfect for: Night owls, extended reading

**3. Light & Airy**
```
Clean and spacious
- Background: White (#ffffff)
- Accent: Blue (#0066cc)
- Font: System default
- Vibe: Professional, easy on eyes
```
Perfect for: Daytime use, sharing

**4. Synthwave**
```
80s retro vibes
- Background: Deep purple (#1e1e2e)
- Accent: Neon pink (#ff79c6)
- Font: Rounded sans-serif
- Vibe: Fun, eye-catching
```
Perfect for: Standing out, creative projects

**5. Nord Theme**
```
Calm and sophisticated
- Background: Blue-grey (#2e3440)
- Accent: Muted blue (#88c0d0)
- Font: Sans-serif
- Vibe: Calm, nordic minimalism
```
Perfect for: Focus, productivity

**6. Solarized Light**
```
Scientifically designed colors
- Background: Cream (#fdf6e3)
- Accent: Blue (#268bd2)
- Font: Monospace
- Vibe: Easy on eyes, balanced
```
Perfect for: Long reading sessions

### ðŸŒˆ Custom Theme

Want something different? Tell me:
- Your favorite colors
- A website you like the look of
- Your company/brand colors
- Or just describe the vibe: "I want it to look like..."

**Just tell me:**
```
"I want [theme name]" or "Make it look like [description]"
```

---

## Step 3: Set Update Frequency

How often should your dashboard fetch fresh data?

### â±ï¸ Frequency Options

**Very Frequent (Real-time feel)**
```
Every 15 minutes - Breaking news, stock prices, live events
Every 30 minutes - Social media, active communities
```
âš ï¸ Higher server load, respect rate limits

**Regular Updates (Recommended)**
```
Every 1 hour   - General news, tech sites (RECOMMENDED)
Every 3 hours  - Blog posts, articles
Every 6 hours  - Shopping deals, releases
```
âœ… Good balance of freshness and efficiency

**Scheduled Updates**
```
Twice daily    - Morning (8am) and Evening (6pm)
Daily at 7am   - Overnight updates, market opens
Weekly         - Fashion drops, curated content
```
ðŸ’¡ Best for content that changes on a schedule

### ðŸ“… Smart Scheduling Tips

**Match the source:**
- Hacker News / Reddit â†’ Every 1-3 hours
- Research papers â†’ Daily
- Fashion drops â†’ Weekly
- Stock prices â†’ Every 15-30 minutes
- Blog posts â†’ Every 6 hours

**Consider your timezone:**
Schedule updates before you typically check the dashboard

**Avoid over-scraping:**
Be respectful of websites - don't hammer them with requests

**Just tell me:**
```
"Update every [frequency]" or "Update daily at [time]"
```

---

## ðŸŽ¯ Quick Examples

Not sure how to phrase it? Here are complete examples:

**Example 1: Tech Enthusiast**
```
"Set up my dashboard with Hacker News, GitHub Trending, and Product Hunt.
Use the dark theme and update every hour."
```

**Example 2: Crypto Trader**
```
"I want to track Bitcoin price from CoinMarketCap, crypto news from Reddit
r/cryptocurrency, and fear & greed index. Synthwave theme, update every 15 minutes."
```

**Example 3: Researcher**
```
"Track arXiv papers in machine learning, Papers with Code trending, and
Google Scholar alerts. Solarized light theme, update daily at 7am."
```

**Example 4: Fashion Fan**
```
"Keep Hypebeast weekly drops, add StockX upcoming releases and Grailed trending.
Light & airy theme, update twice daily."
```

**Example 5: Custom**
```
"I want a dashboard like Hacker News but for design. Track Dribbble popular shots,
Behance trending, and Designer News. Orange theme, update every 3 hours."
```

---

## ðŸ¤– What Happens Next?

Once you tell me what you want, here's what I'll do:

### 1. Build Your Scrapers (2-5 minutes)

**I'll choose the right scraping approach for each website:**

#### ðŸŽ¯ Decision Tree

**Simple API/Fetch (Fastest, Most Reliable)**
```
Use when: Website has a public API or returns clean JSON/HTML
Examples: Hacker News, GitHub API, arXiv RSS
Benefits: Fast, lightweight, no browser needed
Method: Direct HTTP requests with fetch/axios
```
âœ… **Preferred method when available** - no overkill, just clean data extraction

**Headless Browser - Stealth Mode (Advanced Anti-Detection)**
```
Use when: Website has moderate bot detection but loads content via JavaScript
Examples: Product Hunt, Twitter aggregators, Reddit
Benefits: Bypasses basic bot detection, handles dynamic content
Method: playwright-extra + stealth plugin + smart utilities
```
âœ… **Uses pre-installed web-automation skill** with anti-detection features:
- `playwright-extra` with stealth plugin (masks automation fingerprints)
- Human-like interactions (`humanType`, `humanClick`, `randomDelay`)
- Cookie persistence (`saveCookies`/`loadCookies`) for session reuse
- Cloudflare detection (`detectCloudflare`) with 90s auto-solve
- Retry logic with exponential backoff
- Smart timeouts based on traffic patterns

**Visible Browser - Brute Force (Nuclear Option)**
```
Use when: Website has aggressive bot detection (Amazon, Cloudflare, etc.)
Examples: Amazon, heavily protected e-commerce sites
Benefits: Harder to detect (real browser behavior), manual CAPTCHA solving
Method: headless: false + all stealth techniques + human intervention if needed
```
âœ… **Last resort** - visible browser window, you can manually solve CAPTCHAs if needed

**What gets built:**
- Smart scrapers that auto-detect Cloudflare/bot challenges
- Clean JSON data saved to `drops/[source].json`
- Production-ready error handling and logging
- Session persistence (faster subsequent runs)

### 2. Update Your Backend (1 minute)
I'll modify the Flask API to:
- Read your new data sources
- Serve them via REST endpoints
- Handle caching properly

### 3. Update Your Frontend (2 minutes)
I'll update the UI to:
- Display your data beautifully
- Apply your chosen theme
- Format everything nicely (timestamps, numbers, links)

### 4. Set Up Automation (1 minute)
I'll create cron jobs to:
- Run your scrapers automatically
- Update data on your schedule
- Log results for monitoring

### 5. Test Everything (1 minute)
I'll run a test to make sure:
- Scrapers work correctly
- Data displays properly
- Theme looks good
- Refresh button works

**Total time: ~10 minutes**

### 6. Here Are Your Links! ðŸŽ‰

When setup is complete, I'll give you:

**Local Access (same device):**
```
http://localhost:5000
```

**Local Network Access (other devices on your WiFi):**
```
http://192.168.x.x:5000
```
(I'll detect your IP automatically)

**Public HTTPS Access (anywhere on internet):**
```
https://{your-subdomain}.devices.pamir.ai/distiller/proxy/5000/
```
(Works through Distiller proxy - already configured!)

You can bookmark any of these and access your dashboard from anywhere!

---

## ðŸ†˜ Need Help Deciding?

### Not Sure What to Track?

**Ask yourself:**
- What websites do I check every morning?
- What newsletters do I subscribe to?
- What do I Google regularly?
- What would save me time if it was in one place?

**Or ask me:**
```
"What should I track if I'm interested in [topic]?"
"Show me examples of dashboards for [profession/hobby]"
```

### Can't Pick a Theme?

**Ask yourself:**
- When will I use this most? (Day/night)
- What colors do I like?
- Do I want it professional or fun?

**Or just say:**
```
"Surprise me with a theme!" (I'll pick based on your content)
```

### Not Sure About Update Frequency?

**Start with:**
- **Every hour** for most sources (you can change it later)
- **Twice daily** if you only check morning/evening
- **As needed** and just use the manual refresh button

---

## ðŸŽ¬ Ready to Start?

Just say one of these to begin:

```
"I'm ready to customize my dashboard!"
```
(I'll ask you the three questions)

```
"Set up my dashboard with [topics], [theme], and update [frequency]"
```
(I'll do it all at once)

```
"Help me decide what to track"
```
(I'll ask about your interests and suggest sources)

```
"Show me a demo setup"
```
(I'll set up a sample dashboard so you can see how it works)

---

## ðŸ“ Advanced Options

Once your dashboard is running, you can always ask me to:

### Add Features
- "Add search/filter functionality"
- "Add email notifications when certain items appear"
- "Add data export (CSV/JSON)"
- "Add dark/light mode toggle"
- "Track historical data (graphs/charts)"

### Customize Further
- "Change just the colors but keep the layout"
- "Add custom CSS animations"
- "Make the cards bigger/smaller"
- "Add images/thumbnails for items"

### Manage Data
- "Show me the scrapers' debug output"
- "Why isn't [source] updating?"
- "Delete old data older than 7 days"
- "Show me what websites are being slow"

### Scale Up
- "Add 10 more sources"
- "Set different update frequencies per source"
- "Add authentication/password protection"
- "Set up multiple dashboards for different topics"

---

## ðŸ’¬ I'm Here to Help!

Think of me as your pair programmer for this project. Ask me anything:

- âœ… "Can you scrape [specific website]?"
- âœ… "Why is [source] not working?"
- âœ… "How do I change [feature]?"
- âœ… "Make it look more like [website]"
- âœ… "Add a section for [new content type]"

Let's build something awesome together! ðŸš€

---

## ðŸ”§ Technical Reference: Web Automation Skill

### Overview

This project includes a pre-installed **web-automation skill** at `.claude/skills/web-automation/` that provides production-ready browser automation utilities. When building scrapers, I'll automatically use these tools to ensure reliability and anti-detection.

### Why This Skill Exists

**Problem:** Basic Playwright scripts fail on modern websites due to:
- Bot detection (Amazon, Cloudflare, etc.)
- JavaScript-heavy pages that don't render immediately
- Rate limiting and IP blocking
- Cookie/session management complexity

**Solution:** The web-automation skill provides battle-tested utilities from production bots.

### When I Use Each Approach

#### âœ… Simple Fetch/API (No Skill Needed)
```javascript
// Example: Hacker News API
const response = await fetch('https://hacker-news.firebaseio.com/v0/topstories.json');
const data = await response.json();
```
**When:** Site has public API or returns static HTML/JSON
**Speed:** âš¡ Instant (no browser startup)
**Reliability:** â­â­â­â­â­ (no bot detection)

#### âœ… Headless Browser + Stealth (Uses Skill)
```javascript
const { chromium } = require('playwright-extra');
const stealth = require('puppeteer-extra-plugin-stealth')();
const { humanType, detectCloudflare, saveCookies } = require('./.claude/skills/web-automation/lib/utils.js');

chromium.use(stealth);
const browser = await chromium.launch({ headless: true });

// Smart Cloudflare detection
await detectCloudflare(page, context, 'page load', {
  maxWaitSeconds: 90,
  cookiePath: 'session.json'
});

// Human-like typing
await humanType(page, '#search', 'query', { delay: 50 });
```
**When:** Site has moderate bot detection (Product Hunt, Reddit aggregators)
**Speed:** ðŸ¢ ~5-10 seconds (browser startup + page load)
**Reliability:** â­â­â­â­ (bypasses most detection)

#### âœ… Visible Browser + Manual Intervention (Nuclear Option)
```javascript
const browser = await chromium.launch({
  headless: false,  // You can see the browser
  args: ['--disable-blink-features=AutomationControlled']
});

// If CAPTCHA appears, you can solve it manually
console.log('Waiting for manual CAPTCHA solve...');
await page.waitForTimeout(30000);
```
**When:** Site has aggressive detection (Amazon, Cloudflare protected sites)
**Speed:** ðŸŒ ~10-30 seconds (+ manual intervention time)
**Reliability:** â­â­â­â­â­ (humans always win)

### Available Utilities (from web-automation skill)

```javascript
const utils = require('./.claude/skills/web-automation/lib/utils.js');

// Session persistence (avoid repeated logins)
await utils.saveCookies(context, 'session.json');
await utils.loadCookies(context, 'session.json');

// Human-like interactions (avoid bot detection)
await utils.humanType(page, '#input', 'text', { delay: 50 });
await utils.humanClick(page, 'button');
await utils.randomDelay(1000, 2500); // Random wait time

// Smart Cloudflare handling
await utils.detectCloudflare(page, context, 'location', {
  maxWaitSeconds: 90,
  pollIntervalSeconds: 10,
  cookiePath: 'session.json'
});

// Retry logic with exponential backoff
const result = await utils.retryWithBackoff(async () => {
  await page.waitForSelector('.data', { timeout: 10000 });
  return await page.textContent('.data');
}, { maxRetries: 3 });

// Safe interactions with auto-retry
await utils.safeClick(page, 'button', { retries: 3, timeout: 10000 });
await utils.captureScreenshot(page, 'debug.png');
```

### What Makes Scrapers "Production-Ready"

When I build scrapers using the web-automation skill, they include:

1. **Anti-Detection**
   - `playwright-extra` with stealth plugin (masks automation fingerprints)
   - Human-like typing/clicking with random delays
   - Realistic user agents and viewport sizes

2. **Error Handling**
   - Automatic retries with exponential backoff
   - Screenshots captured on failures
   - Graceful degradation (partial data better than no data)

3. **Performance**
   - Cookie persistence (skip login flows on subsequent runs)
   - Smart timeouts (longer during peak traffic hours)
   - Parallel scraping where possible

4. **Bot Protection**
   - Cloudflare auto-detection with 90s polling
   - Block tracking (logs consecutive/total blocks)
   - Automatic session persistence after challenge clears

5. **Observability**
   - Detailed console logging with timestamps
   - JSON output for easy debugging
   - Error screenshots saved automatically

### Example: How I Choose the Approach

**User Request:** "Track Product Hunt, Amazon deals, and GitHub trending"

**My Decision Process:**

1. **GitHub Trending** â†’ Simple Fetch/API
   - GitHub has a trending page with clean HTML
   - No bot detection
   - Fast and reliable
   - âœ… Use basic fetch or axios

2. **Product Hunt** â†’ Headless + Stealth
   - JavaScript-heavy page
   - Moderate bot detection
   - Data loads dynamically
   - âœ… Use playwright-extra + stealth + utilities

3. **Amazon** â†’ Visible Browser (if needed)
   - Aggressive bot detection
   - Cloudflare challenges common
   - May require CAPTCHA solving
   - âœ… Start with headless + stealth, fallback to headless: false if blocked

### Files Structure

```
.claude/skills/web-automation/
â”œâ”€â”€ SKILL.md                    # Full documentation
â”œâ”€â”€ lib/
â”‚   â””â”€â”€ utils.js                # Production utilities I'll use
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ login-flow.js           # Session persistence example
â”‚   â”œâ”€â”€ cloudflare-handler.js   # Bot protection example
â”‚   â””â”€â”€ data-extraction.js      # Scraping with retry logic
â””â”€â”€ node_modules/
    â”œâ”€â”€ playwright-extra/       # Enhanced Playwright
    â””â”€â”€ puppeteer-extra-plugin-stealth/  # Stealth plugin
```

### Why You Don't Need to Worry

When you ask me to scrape a website:
1. I automatically check if `.claude/skills/web-automation/` exists
2. I choose the right approach (fetch vs headless vs visible)
3. I use the utilities to handle edge cases
4. I test the scraper and show you the results

You just tell me **what** to scrape, and I handle **how** to scrape it reliably.

---

**So... what would you like to track?** ðŸ‘€
